{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import datasets, transforms\n",
    "import lightning.pytorch as lightning\n",
    "from lightning.pytorch.core import LightningModule\n",
    "\n",
    "class LightningCifarClassifier(LightningModule):\n",
    "\n",
    "  def __init__(self, arch=\"resnet18\"):\n",
    "    super(LightningCifarClassifier, self).__init__()\n",
    "    self.save_hyperparameters()\n",
    "    self.model = nn.Sequential(\n",
    "        # Conv2d( in_channels, out_channels, kernel_size) B,3,32,32\n",
    "        nn.Conv2d(3, 32, 3, stride= 2, padding=1),  nn.BatchNorm2d(32), nn.Tanh(), # B,32,16,16\n",
    "        nn.Conv2d(32, 64, 3, stride= 2, padding=1), nn.BatchNorm2d(64), nn.Tanh(), # B,32,8,8\n",
    "        nn.Conv2d(64, 128, 3, stride= 2, padding=1), nn.BatchNorm2d(128), nn.Tanh(), # B,32,4,4\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(4*4*128, 64), nn.BatchNorm1d(64), nn.Tanh(),\n",
    "        nn.Linear(64, 10)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "      logits = self.model(x)\n",
    "      return logits\n",
    "\n",
    "  def training_step(self, train_batch, batch_idx):\n",
    "      x, y = train_batch\n",
    "      logits = self.forward(x)   # we already defined forward and loss in the lightning module. We'll show the full code next\n",
    "      loss = F.cross_entropy(logits,y)\n",
    "      self.log('train_loss',loss)\n",
    "\n",
    "      return {'loss': loss}\n",
    "\n",
    "  def validation_step(self, val_batch, batch_idx):\n",
    "      x, y = val_batch\n",
    "      logits = self.forward(x)\n",
    "      loss = F.cross_entropy(logits,y)\n",
    "      self.log('val_loss', loss)\n",
    "      self.log('avg_val_loss', loss, on_epoch=True)\n",
    "\n",
    "      return {'val_loss': loss}\n",
    "\n",
    "\n",
    "  def prepare_data(self):\n",
    "    torchvision.datasets.CIFAR10(root='./data', download=True, train=True)\n",
    "    torchvision.datasets.CIFAR10(root='./data', download=True, train=False)\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    transform = transforms.Compose(\n",
    "    [\n",
    "      transforms.RandomHorizontalFlip(0.5),\n",
    "      transforms.ColorJitter(0.2,0.2,0.2),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "    ])\n",
    "    cifar10 = torchvision.datasets.CIFAR10(root='./data', download=False, train=True, transform=transform)\n",
    "\n",
    "    dataloader = DataLoader(cifar10, batch_size=32, shuffle=True, num_workers=2)\n",
    "    return dataloader\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "        ])\n",
    "    cifar10_test = torchvision.datasets.CIFAR10(root='./data', download=False, train=False, transform=test_transform)\n",
    "    dataloader_test = DataLoader(cifar10_test, batch_size=32, shuffle=False, num_workers=2)\n",
    "    return dataloader_test\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    # the lightningModule HAS the parameters (remember that we had the __init__ and forward method but we're just not showing it here)\n",
    "\n",
    "    optimizer = torch.optim.Adam(self.parameters(),0.001)\n",
    "    # optimizer =  torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.00001)\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
